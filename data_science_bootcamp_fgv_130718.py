# -*- coding: utf-8 -*-
# Data Science_Bootcamp_FGV_130718.ipynb

# Automatically generated by Colaboratory.

# Eduardo Aydar de Oliveira

# Cálculo das estimativas das notas - ENEM

# 1. Carregamento dos Dados
# Download dos arquivos de dados das escolas, e carga no ambiente.

# 1.1 Importação das Lib's necessárias**

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://apache.osuosl.org/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz
!tar xf spark-2.2.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.2.1-bin-hadoop2.7"

!pip install -U -q PyDrive

# pydrive documentation: https://pythonhosted.org/PyDrive/filelist.html

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
 
# 1.2 Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# 1.3 Add files to Collaboratory
escolas_list = '1i-DzRlw6LzCttSt5d1QrPIlHyMR1td4V'
escolas_list = drive.CreateFile({'id': escolas_list})
escolas_list.GetContentFile('ENEM2015.csv')

# 1.4 Add files to Collaboratory
escolas_enem = '1U9eBMS6bsDWCHxpqXQ6DZQOtwrCnuVeV'
escolas_enem = drive.CreateFile({'id': escolas_enem})
escolas_enem.GetContentFile('ESC2013_RMSP_CEM.csv')

!ls

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

from operator import add
from pyspark import SparkContext

# 1.5 Carregamento dos Dados das escolas

from pyspark.sql.types import *
from pyspark.sql.functions import *
import math

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

sc = SparkContext.getOrCreate()

escolas = spark.read.csv('ENEM2015.csv', inferSchema=True, header=True)
type(escolas)
escolas.take(10)

lista = spark.read.csv('ESC2013_RMSP_CEM.csv', inferSchema=True, header=True)
type(lista)
lista.take(10)

# 2. Seleção das Colunas do Dataset

# Utilizado o código da escola e a classificação em 2015,  para comparação com o modelo estimado.

esc_df = escolas.select("CODIGO DA ENTIDADE", "Classificacao")
esc_df = esc_df.toDF("COD", "Classificacao")
esc_df.take(10)

# Utilizadas as médias das notas anuais das escolas (2007, 2009, 2011, 2013) para determinação da média estimada para 2015.

list_df = lista.select("CODESC", "NP07_AF", "NP09_AF", "NP11_AF", "NP13_AF")
list_df = list_df.toDF("COD", "NP07_AF", "NP09_AF", "NP11_AF", "NP13_AF")
list_df.take(10)

# 2.2 Limpeza dos Dados

# 2.2.1 Identificação de Dados Duplicados

esc_df.count() - esc_df.dropDuplicates().count()

list_df.count() - list_df.dropDuplicates().count()

# 2.2.2 Identificação de Dados faltantes"""

esc_df.count() - esc_df.dropDuplicates().dropna(how="any", subset=["COD", "Classificacao"]).count()

list_df.count() - list_df.dropDuplicates().dropna(how="any", subset=["NP07_AF", "NP09_AF", "NP11_AF", "NP13_AF"]).count()

# 2.3 Preenchimento dos dados faltantes"""

esc_df = esc_df.dropDuplicates().fillna(value=0, subset=["CODIGO DA ENTIDADE", "Classificacao"])
esc_df.count()

list_df = list_df.dropDuplicates().fillna(value=0, subset=["NP07_AF", "NP09_AF", "NP11_AF", "NP13_AF"])
list_df.count()

# 2.4 JOIN dos Dados
# Efetuado o join dos dados de 2015 com os dados anuais, utilizando-se como chave o código da escola.

comb = esc_df.join(list_df, "COD", "left")

comb.show()

comb

comb.describe()

comb.count()

# 3. Criação de um Modelo de Classificação

# 3.1 Montagem dos dados de Treinamento / Teste
"""
30% = Training Data
70% = Test Data
"""

splits = comb.randomSplit([0.3, 0.7])
treino = splits[0]
teste = splits[1]
treino_rows = treino.count()
teste_rows = teste.count()
print "Treino Rows:", treino_rows, " Teste Rows:", teste_rows

treino

# 3.1.1 Preparação dos Dados de Treinamento
# Utilização de classe  **VectorAssembler** para transformar as colunas das médias anuais em um vetor, e renomear a coluna "Classificação".

assembler = VectorAssembler(inputCols = ["NP07_AF", "NP09_AF", "NP11_AF", "NP13_AF"], outputCol="hist")
training = assembler.transform(treino).select(col("hist"), (col("Classificacao").cast("Int").alias("predicao")))
training.show()

# 3.1.2 Treino de um Modelo de Classificação
# Utilizada a Regressão Logística.

lr = LogisticRegression(labelCol="predicao",featuresCol="hist",maxIter=10,regParam=0.3)
model = lr.fit(training)
print "Model trained!"

# 3.2 Preparando os Dados de Teste

teste_mod = assembler.transform(teste).select(col("hist"), (col("Classificacao")).cast("Int").alias("real"))
teste_mod.show()

# 3.3 Teste do Modelo

testando = model.transform(teste_mod)
estimado = testando.select("hist", "prediction", "real")
estimado.show(100)

type(estimado)

est_rd = estimado.select(round("prediction", 0).alias("Estimativa"), "real")
est_rd.show()
est_rd.count()

resultado = est_rd.withColumn("res", when (est_rd.Estimativa == est_rd.real, True).otherwise(False))
resultado.show()
resultado.count()

cal = resultado.where(col("res") == False)
cal.count()

# 4. Comparação Final

((894-298)*100)/894

# 5. Conclusão: 
# O percentual de acerto, na primeira execução foi de 66%. Com mais interações e uso de mais variáveis os cálculos serão refinados para aperfeiçoar o resultado.

### Att,
# Eduardo
